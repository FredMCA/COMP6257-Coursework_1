% LaTeX Template for short student reports.
% Citations should be in bibtex format and go in references.bib
\documentclass[a4paper, 11pt]{article}
\usepackage[top=3cm, bottom=3cm, left = 2cm, right = 2cm]{geometry} 
\geometry{a4paper} 
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{graphicx} 
\usepackage{amsmath,amssymb}  
\usepackage{bm}  
\usepackage[pdftex,bookmarks,colorlinks,breaklinks]{hyperref}  
%\hypersetup{linkcolor=black,citecolor=black,filecolor=black,urlcolor=black} % black links, for printed output
\usepackage{memhfixc} 
\usepackage{pdfsync}  
\usepackage{fancyhdr}
\usepackage{mathpazo}
\pagestyle{fancy}

\title{COMP6257 Coursework 1}
\author{Frederik McArthur, fsm1g19@soton.ac.uk}
%\date{}

\begin{document}
\maketitle
\tableofcontents

\section{Introduction}
\label{section:partitionedmatrix}

My project was about \ldots

I developed a system to \ldots

We did some experiments to find out \ldots

The main results were \ldots

\pagebreak

\section{Verifying the Inverse of a Partitioned Matrix}

The inverse of a partitioned matrix is given by:
$$
\begin{bmatrix}
    A & B \\
    C & D
\end{bmatrix}^{-1}
= \begin{bmatrix}
    M & -MBD^{-1} \\
    -D^{-1}CM & D^{-1} + D^{-1}CMBD^{-1}
\end{bmatrix}
    $$
Where $M = \begin{bmatrix}
    A - BD^{-1}C
\end{bmatrix}^{-1}$. To verify that $\begin{bmatrix}
    M & -MBD^{-1} \\
    -D^{-1}CM & D^{-1} + D^{-1}CMBD^{-1}
\end{bmatrix}$ is the inverse of $\begin{bmatrix}
    A & B \\
    C & D
\end{bmatrix}$, the following condition must be met:
$$
\begin{bmatrix}
    A & B \\
    C & D
\end{bmatrix} * 
\begin{bmatrix}
    M & -MBD^{-1} \\
    -D^{-1}CM & D^{-1} + D^{-1}CMBD^{-1}
\end{bmatrix}
= \begin{bmatrix}
    I & 0 \\
    0 & I
\end{bmatrix}$$
Examining each element of the matrix, and using the knowledge that $A*I = I*A = A$, and $A*A^{-1} = A^{-1}A = I$;
\begin{itemize}
    \item Row 1 Column 1;
    \begin{align*}
        AM + B(-D^{-1}CM) &= I\\
        AM - BD^{-1}CM &= I\\
        M * (A - BD^{-1}C) &= I\\
        M * M^{-1} &= I
    \end{align*}
    \item Row 1 Column 2;
    \begin{align*}
        A (-MBD^{-1}) + B(D^{-1} + D^{-1}CMBD^{-1}) &= 0 \\
        -AMBD^{-1} +BD^{-1} + BD^{-1}CMBD^{-1} &= 0\\
        (AM + I + BD^{-1}CM)BD^{-1} &= 0\\
        ((A-BD^{-1}C)M - I)BD^{-1} &= 0 \\
        (M^{-1}M - I)BD^{-1} &= 0 \\
        (I - I) BD^{-1} &= 0
    \end{align*}
    \item Row 2 Column 1;
    \begin{align*}
        CM + D(-D^{-1}CM) &= 0 \\
        CM - DD^{-1}CM &= 0 \\
        CM - ICM &= 0\\
        CM - CM &= 0
    \end{align*}
    \item and finally, Row 2 Column 2;
    \begin{align*}
        C(-MBD^{-1}) + D(D^{-1} + D^{-1}CMBD^{-1}) &= I \\
        -CMBD^{-1} +  DD^{-1} + DD^{-1}CMBD^{-1} &= I \\
        -CMBD^{-1} + I + ICMBD^{-1} &= I \\
        -CMBD^{-1} + CMBD^{-1} + I &= I \\
        I &= I
    \end{align*}
\end{itemize}
This demonstrates that $
\begin{bmatrix}
    A & B \\
    C & D
\end{bmatrix}^{-1}
= \begin{bmatrix}
    M & -MBD^{-1} \\
    -D^{-1}CM & D^{-1} + D^{-1}CMBD^{-1}
\end{bmatrix}
    $.


    
\pagebreak

\section{Inverse of a rank one update of a matrix}

To demonstrate that a matrix is the inverse, as demonstrated in Section \ref{section:partitionedmatrix}, the $A*I = I*A = A$ identity can be used to prove this. Therefore, when examining 

$$ [A + xx^T ]^{-1} = A^{-1} - \frac{A^{-1}xx^TA^{-1}}{1 + x^TA^{-1}x}$$

we can prove that $\frac{A^{-1}xx^TA^{-1}}{1 + x^TA^{-1}x}$ is the inverse by the following process;

\begin{align*}
    I &= [A + xx^T ] \left(A^{-1} - \frac{A^{-1}xx^TA^{-1}}{1 + x^TA^{-1}x}\right) \\
    I &= AA^{-1} + xx^TA^{-1} - \frac{AA^{-1}xx^TA^{-1} + xx^TA^{-1}xx^TA^{-1}}{1 + x^TA^{-1}x}\\
    I &= I + xx^TA^{-1} - \frac{xx^TA^{-1} + xx^TA^{-1}xx^TA^{-1}}{1+x^TA^{-1}x} \\
    I &= I + xx^TA^{-1} - \frac{x(1+x^TA^{-1}x)x^TA}{1+x^TA^{-1}x} \\
    I &= I + xx^TA^{-1} - xx^TA^{-1} \\
    I &= I
\end{align*}
 Thus, proving that the inverse of a Rank one update of a matrix is given by 
$$ [A + xx^T ]^{-1} = A^{-1} - \frac{A^{-1}xx^TA^{-1}}{1 + x^TA^{-1}x}$$

\section{Bayesian Estimation}

Equations 2.141 (Equation \ref{eq:prml2141}) and 2.142 (Equation \ref{eq:prml2142}) in Pattern Recognition and Machine Learning are given as follows:

\begin{equation} \label{eq:prml2141}
\mu_N = \frac{\sigma^2}{N\sigma_0^2 + \sigma^2}\sigma_0 + \frac{N\sigma^2}{N\sigma_0^2 + \sigma^2}\sigma_{ML}
\end{equation}

\begin{equation} \label{eq:prml2142}
\frac{1}{\sigma^2_N} = \frac{1}{\sigma^2_0} + \frac{N}{\sigma^2}
\end{equation}

\subsection{When $N \rightarrow \infty$}

When $N$ tends towards infinity, Equation \ref{eq:prml2141} will tend towards the maximum likelihood solution, as $\frac{\sigma^2}{N\sigma_0^2 + \sigma^2}$ will tend towards zero, leaving $\mu_N = \frac{N\sigma^2}{N\sigma_0^2 + \sigma^2}\sigma_{ML}$. 
Similarly, in Equation \ref{eq:prml2141}, the variance will tend towards zero, as the right hand side of the equation will tend towards infinity, and $\frac{1}{\infty} \rightarrow 0$.

\subsection{A highly confident Prior}

For a highly confident prior, the variance would tend towards zero, so $\sigma_0 \rightarrow 0$. This would mean that the mean would reduce to the prior value, and not the maximum likelihood value. 

\section{Bayesian Analysis of the Illustrative Polynomial}

The dataset generated for this problem was generated with Numpy, first by generating a uniform dataset, and then by applying a sinusoidal function to it. The Sklearn \verb|train_test_split| function is then used to split the dataset into both a training and a testing dataset. A plot of the training and testing dataset can be seen in Figure \ref{fig:traintestsplit}.

\begin{figure}[h]
    \includegraphics[width=8cm]{fig/traintestsplit.png}
    \caption{A figure showing the training and testing datasets.}
    \label{fig:traintestsplit}
\end{figure}

Polynomial regression was then performed on the data, with trials at different P values. As shown in Figure \ref{fig:pvalueboxplt} shows that as the P value increases, the accuracy increases towards a point. Beyond a certain value, the graph will begin to over fit.

\begin{figure}[h]
    \includegraphics[width=8cm]{fig/pvalueboxplot.png}
    \caption{A figure showing the error values for different p values.}
    \label{fig:traintestsplit}
\end{figure}

\pagebreak

\section{Evaluation}

We did some experiments \ldots

\pagebreak

\section{Conclusions and Future Work}

From our experiments we can conclude that \ldots

\bibliographystyle{abbrv}
% \bibliography{references}  % need to put bibtex references in references.bib 
\end{document}