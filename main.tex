% LaTeX Template for short student reports.
% Citations should be in bibtex format and go in references.bib
\documentclass[a4paper, 11pt]{article}
\usepackage[top=3cm, bottom=3cm, left = 2cm, right = 2cm]{geometry} 
\geometry{a4paper} 
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{graphicx} 
\usepackage{amsmath,amssymb}  
\usepackage{bm}  
\usepackage[pdftex,bookmarks,colorlinks,breaklinks]{hyperref}  


\documentclass{article}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
%\hypersetup{linkcolor=black,citecolor=black,filecolor=black,urlcolor=black} % black links, for printed output
\usepackage{memhfixc} 
\usepackage{pdfsync}  
\usepackage{fancyhdr}
\usepackage{mathpazo}
\pagestyle{fancy}

\title{COMP6257 Coursework 1}
\author{Frederik McArthur, fsm1g19@soton.ac.uk}
%\date{}

\begin{document}
\maketitle
\tableofcontents


\section{Verifying the Inverse of a Partitioned Matrix}
\label{section:partitionedmatrix}
The inverse of a partitioned matrix is given by:
$$
\begin{bmatrix}
    A & B \\
    C & D
\end{bmatrix}^{-1}
= \begin{bmatrix}
    M & -MBD^{-1} \\
    -D^{-1}CM & D^{-1} + D^{-1}CMBD^{-1}
\end{bmatrix}
    $$
Where $M = \begin{bmatrix}
    A - BD^{-1}C
\end{bmatrix}^{-1}$. To verify that $\begin{bmatrix}
    M & -MBD^{-1} \\
    -D^{-1}CM & D^{-1} + D^{-1}CMBD^{-1}
\end{bmatrix}$ is the inverse of $\begin{bmatrix}
    A & B \\
    C & D
\end{bmatrix}$, the following condition must be met:
$$
\begin{bmatrix}
    A & B \\
    C & D
\end{bmatrix} * 
\begin{bmatrix}
    M & -MBD^{-1} \\
    -D^{-1}CM & D^{-1} + D^{-1}CMBD^{-1}
\end{bmatrix}
= \begin{bmatrix}
    I & 0 \\
    0 & I
\end{bmatrix}$$
Examining each element of the matrix, and using the knowledge that $A*I = I*A = A$, and $A*A^{-1} = A^{-1}A = I$;
\begin{itemize}
    \item Row 1 Column 1;
    \begin{align*}
        AM + B(-D^{-1}CM) &= I\\
        AM - BD^{-1}CM &= I\\
        M * (A - BD^{-1}C) &= I\\
        M * M^{-1} &= I
    \end{align*}
    \item Row 1 Column 2;
    \begin{align*}
        A (-MBD^{-1}) + B(D^{-1} + D^{-1}CMBD^{-1}) &= 0 \\
        -AMBD^{-1} +BD^{-1} + BD^{-1}CMBD^{-1} &= 0\\
        (AM + I + BD^{-1}CM)BD^{-1} &= 0\\
        ((A-BD^{-1}C)M - I)BD^{-1} &= 0 \\
        (M^{-1}M - I)BD^{-1} &= 0 \\
        (I - I) BD^{-1} &= 0
    \end{align*}
    \item Row 2 Column 1;
    \begin{align*}
        CM + D(-D^{-1}CM) &= 0 \\
        CM - DD^{-1}CM &= 0 \\
        CM - ICM &= 0\\
        CM - CM &= 0
    \end{align*}
    \item and finally, Row 2 Column 2;
    \begin{align*}
        C(-MBD^{-1}) + D(D^{-1} + D^{-1}CMBD^{-1}) &= I \\
        -CMBD^{-1} +  DD^{-1} + DD^{-1}CMBD^{-1} &= I \\
        -CMBD^{-1} + I + ICMBD^{-1} &= I \\
        -CMBD^{-1} + CMBD^{-1} + I &= I \\
        I &= I
    \end{align*}
\end{itemize}
This demonstrates that $
\begin{bmatrix}
    A & B \\
    C & D
\end{bmatrix}^{-1}
= \begin{bmatrix}
    M & -MBD^{-1} \\
    -D^{-1}CM & D^{-1} + D^{-1}CMBD^{-1}
\end{bmatrix}
    $.



\section{Inverse of a rank one update of a matrix}

To demonstrate that a matrix is the inverse, as demonstrated in Section \ref{section:partitionedmatrix}, the $A*I = I*A = A$ identity can be used to prove this. Therefore, when examining 

$$ [A + xx^T ]^{-1} = A^{-1} - \frac{A^{-1}xx^TA^{-1}}{1 + x^TA^{-1}x}$$

we can prove that $\frac{A^{-1}xx^TA^{-1}}{1 + x^TA^{-1}x}$ is the inverse by the following process;

\begin{align*}
    I &= [A + xx^T ] \left(A^{-1} - \frac{A^{-1}xx^TA^{-1}}{1 + x^TA^{-1}x}\right) \\
    I &= AA^{-1} + xx^TA^{-1} - \frac{AA^{-1}xx^TA^{-1} + xx^TA^{-1}xx^TA^{-1}}{1 + x^TA^{-1}x}\\
    I &= I + xx^TA^{-1} - \frac{xx^TA^{-1} + xx^TA^{-1}xx^TA^{-1}}{1+x^TA^{-1}x} \\
    I &= I + xx^TA^{-1} - \frac{x(1+x^TA^{-1}x)x^TA}{1+x^TA^{-1}x} \\
    I &= I + xx^TA^{-1} - xx^TA^{-1} \\
    I &= I
\end{align*}
 Thus, proving that the inverse of a Rank one update of a matrix is given by 
$$ [A + xx^T ]^{-1} = A^{-1} - \frac{A^{-1}xx^TA^{-1}}{1 + x^TA^{-1}x}$$

\section{Bayesian Estimation}

Equations 2.141 (Equation \ref{eq:prml2141}) and 2.142 (Equation \ref{eq:prml2142}) in Pattern Recognition and Machine Learning are given as follows:

\begin{equation} \label{eq:prml2141}
\mu_N = \frac{\sigma^2}{N\sigma_0^2 + \sigma^2}\sigma_0 + \frac{N\sigma^2}{N\sigma_0^2 + \sigma^2}\sigma_{ML}
\end{equation}

\begin{equation} \label{eq:prml2142}
\frac{1}{\sigma^2_N} = \frac{1}{\sigma^2_0} + \frac{N}{\sigma^2}
\end{equation}

\subsection{When $N \rightarrow \infty$}

When $N$ tends towards infinity, Equation \ref{eq:prml2141} will tend towards the maximum likelihood solution, as $\frac{\sigma^2}{N\sigma_0^2 + \sigma^2}$ will tend towards zero, leaving $\mu_N = \frac{N\sigma^2}{N\sigma_0^2 + \sigma^2}\sigma_{ML}$. 
Similarly, in Equation \ref{eq:prml2141}, the variance will tend towards zero, as the right hand side of the equation will tend towards infinity, and $\frac{1}{\infty} \rightarrow 0$.

\subsection{A highly confident Prior}

For a highly confident prior, the variance would tend towards zero, so $\sigma_0 \rightarrow 0$. This would mean that the mean would reduce to the maximum likelehood value, as $\sigma_0 \rightarrow 0$. The variance would also tend towards infinity, as $1/0 \rightarrow \infty$

\section{Bayesian Analysis of the Illustrative Polynomial}

\subsection{Generating the dataset}
The dataset generated for this problem was generated with Numpy, first by generating a uniform dataset, and then by applying a sinusoidal function to it. The Sklearn \verb|train_test_split| function is then used to split the dataset into both a training and a testing dataset. A plot of the training and testing dataset can be seen in Figure \ref{fig:traintestsplit}.

\subsubsection{Performing Polynomial Regression}

\begin{figure}[h]
    \begin{center}
    \includegraphics[width=8cm]{fig/traintestsplit.png}
    \caption{A figure showing the training and testing datasets.}
    \label{fig:traintestsplit}
\end{center}
\end{figure}

Polynomial regression was then performed on the data, with trials at different P values. As shown in Figure \ref{fig:pvalueboxplt} shows that as the P value increases, the accuracy increases towards a point. Beyond a certain value, the graph will begin to over fit, and the performance on an unknown dataset will then decrease.

\begin{figure}[h]
    \begin{center}
    \includegraphics[width=8cm]{fig/pvalueboxplot.png}
    \caption{A figure showing the error values for different p values.}
    \label{fig:pvalueboxplt}     
\end{center}
\end{figure}

\subsection{Regularised Regression for a High Order Polynomial}
\label{sec:changinglamda}

Examining the polynomial at $p=9$, the regularisation hyperparameter, $\lambda$ has an effect on the accuracy of the model. Generally, for lower values of $\lambda$, the error rate is high, before hitting a minimum at approximaley $ln(\lambda) = -18$, before increasing again. This is the same as the value achieved in C Bishop, PRML. This can also be seen in Figure \ref{fig:pandlamchange}
\begin{figure}[h]
    \begin{center}
    \includegraphics[width=16cm]{fig/displaypvalueplot.png}
    \caption{A figure showing the error for the P values on the left (where $\lambda=0$), and a range of $\lambda$ values (where $p=9$) on the right.}
    \label{fig:pandlamchange}     
\end{center}
\end{figure}

\subsection{Bayesian Evidence Formula at different $\alpha$ and $\beta$ values}
To show the change in probability over different a and b values, I have plotted a contour showing this in Figure \ref*{fig:abcontourplot}. This plot shows that as alpha and beta increases, the evidence value increases for the given dataset.
\begin{figure}[h]
    \begin{center}
    \includegraphics[width=16cm]{fig/contourplot.png}
    \caption{A figure showing the log value of the probability over a range of $\alpha$ and $\beta$ values.}
    \label{fig:abcontourplot}     
\end{center}
\end{figure}

\subsection{Find the values of $\alpha$ and $\beta$ that maximise $p(t|\alpha, \beta)$}

I attempted various approaches to finding the maximum evidence value.

I attempted to work through the equations shown in Bishop, Section 3.5.2 - Maximising the Evidence Function, implementing Equations 3.87, 3.92, 3.91 and 3.95, by implementing the following loop:
\begin{itemize}
    \item Estimate a value for $\alpha$ and $\beta$.
    \item For N iterations,\begin{itemize}
        \item Calculate $\lambda_i$ by calculating the eigenvalue from $(\beta \Phi ^T \Phi) u_i= \lambda_i u_i $ (Eq 3.87).
        \item Calculate $\alpha$ firstly using $\alpha = \frac{\gamma}{m^T_N m_N}$, and after this did not work, using the fact that A has Eigenvalues $\alpha + \lambda_i$.
        \item Calculate $\gamma$ using $\gamma = \sum_i \frac{\lambda_i}{\alpha + \lambda_i}$
        \item Calculating $\beta$ using $\beta = \frac{1}{\frac{1}{N - \gamma} \sum_{n=1}^{N} {t_n - m_N^T \phi(x_n)}^2}$, and reusing this $\beta$ value in the next iteration.
    \end{itemize}
\end{itemize}

This unfortunatley did not produce the expected values, producing very low $\alpha$ and $\beta$ values.

I also attempted to find the maximum value by calculating a range, and selecting the maximum $\alpha$ and $\beta$ values, but this again did not work as when $\alpha$ or $\beta$ increases, the evidence value increases.

For this reason, I have elected to use the value provided in Bishop, $\alpha = 5x10^{-3}$ to continue. 



\subsection{Proximity to the best $\lambda$ value Found in Section \ref{sec:changinglamda}}

With the $\alpha$ value selected to be $\alpha = 5e{-3}$, and the previous calculated regularisation parameter to be $ln(\lambda) = -18$, or $\lambda = e^{-18} = 1.523e-08$, as $\frac{\alpha}{\beta}$ is analogous to the regularisation parameter, $\lambda$, the value of $\beta$ can be approximated to $\frac{\alpha}{\lambda}$ as follows;

$$\beta = \frac{\alpha}{\lambda} = \frac{5e{-3}}{1.523e^{-8}} = 65659.96$$

\pagebreak

\section{Robot Navigation Problem}

For the sampling problems, I created a mixture distribution from the equations provided in the coursework. These distributions were defined as:



\begin{equation}\label{eq:gauss1}
    \begin{align}
        p_1(x) &= \mathcal{N}(x|\mu_1, \sigma_1) \; \; where \; \mu_1 = \begin{bmatrix}2.0 \ 5.0\end{bmatrix},  \; \sigma_1 = \begin{bmatrix}2 & 1 \ 1 & 2\end{bmatrix} \\
        p_2(x) &= \mathcal{N}(x|\mu_2, \sigma_2) \; \; where \; \mu_2 = \begin{bmatrix}3.0 \ 1.0\end{bmatrix},  \; \sigma_2 = \begin{bmatrix}0.1 & 0 \ 0 & 0.1\end{bmatrix} = 0.1 \mathcal{I} \\
        p(x) &= p_1(x) + p_2(x)
    \end{align}
\end{equation}

with the mixture distributions defined as $ p(x) = p_1(x) + p_2(x)$. 

I also created a distribution for use in both Rejection and Importance sampling, defined as 


\begin{equation}\label{eq:dist}
        q(x) = \mathcal{N}(x|\mu_q, \sigma_q) \: where \: \mu_q = \begin{bmatrix}2.5 \ 4.0\end{bmatrix}, \sigma_q = \begin{bmatrix}3 & 0.2 \ 0.3 & 4\end{bmatrix}
\end{equation}
where the two distributions were enclosed as much as reasonably possible. These distributions are visible in Figure \ref*{fig:distributions}.

\begin{figure}[h]
    \begin{center}
    \includegraphics[width=8cm]{fig/distributionsplotted.jpg}
    \caption{A figure showing distribution 1, 2 and the enclosing normal distribution. Distribution $p_1$ is the oval shaped distribution in the top left, and distribution $p_2$ is the small distribution in the bottom right. Distribution 3 is the enclosing distribution.}
    \label{fig:distributions}
\end{center}
\end{figure}

\subsection{Rejection Sampling}

\begin{figure}[h]
    \begin{center}
    \includegraphics[width=8cm]{fig/contour.png}
    \caption{A figure showing the three contours, and plots of the samples which were calculated in the Rejection sampling processs. Points in orange are rejected, and points in blue are accepted. The sample size is $n=100$}
    \label{fig:rejectionsampling}
\end{center}
\end{figure}
To perform rejection sampling, the normal distribution shown in Equation \ref{eq:dist} is created for a set number of samples. Each sample is then iterated over, with the probability of the mixed distribution calculated. If the combined probability is less than the probability of the q distribution, then the sample value is accepted.

The output of the Rejection sampling algorithm can be seen in Figure \ref*{fig:rejectionsampling}.



The code for the rejection sampling can be found below:
\begin{lstlisting}[language=Python, caption=Rejection Sampling Code.]
def rejection(q_mean, q_var, q, k, dist1, dist2, w, n = 1000, 
    forced_input = False, forced_input_value = None):

    #   Defining the varibles for accepted and rejected variables.
    accepted, z_accepted, rejected, z_rejected = [],[],[],[] 

    #   Creating the samples from the provided distribution information.
    samples = np.random.multivariate_normal(q_mean, q_var, n)

    #   If the input is forced, set the samples to be the input. Used for 
    # calculating the probability for a specific value.
    if forced_input:
        samples = forced_input_value

    #   Iterating over the samples.
    for sample in samples:
        #   Calculating the mixed gaussian probability dist,
        # with the weights.
        prob_dist_1 = w[0]*dist1(sample)
        prob_dist_2 = w[1]*dist2(sample)
        combined_prob = prob_dist_1 + prob_dist_2
    
        y = k * q(sample)
    
        #   If the combined prob is less than the q dist prob
        if combined_prob > y:
            # Accept result
            accepted.append(sample)
            z_accepted.append(y)
        else:
            # else, reject the result
            rejected.append(sample)
            z_rejected.append(y)

    # Return an array of results.
    return np.array(accepted),np.array(z_accepted), 
            np.array(rejected), np.array(z_rejected)
\end{lstlisting}

\subsection{Importance Sampling}

Importance sampling was performed by calculating the following value:

\begin{equation}\label{eq:importance}
        \mathcal{E}[x] = \frac{1}{n} \sum_{i=0}^n x_i \frac{p(x_i)}{q(x_i)} q(x_i)
\end{equation}

Where $\frac{p(x_i)}{q(x_i)} q(x_i)$ are the importance weights of the problem. 

This calculation prpvodes the probability of the selection of the value in the mixture distribution defined in Equation \ref*{eq:gauss1}. The code for this can be found below:
\begin{lstlisting}[language=Python, caption=Importance Sampling Code.]
    def importance(q, w, dist1, dist2, vals):
        #   Calculate the weights from the distributions
        weights = ((w[0] * dist1(vals)) + (w[1] * dist2(vals))) / q(vals)
        weights = weights * q(vals)

        #   Calculate the probability by multiplying by the weights.
        y = np.sum(weights*vals)/len(vals)

        #   Return the probability.
        return y
\end{lstlisting}

\subsection{Comparison between results}

Examining the point $x = [4.0, 4.0]^T$, importance sampling and rejection sampling score similarly. Importance sampling produces a probability of $p_i = 0.032078$, where rejection sampling produces a probability of $p_r = 0.031606$. These probabilities are very similar, with each algorithm rejecting the position. 
\pagebreak



\bibliographystyle{abbrv}
% \bibliography{references}  % need to put bibtex references in references.bib 
\end{document}